{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Begin\n",
    "\n",
    "This is a practical part of your ASR-TTS course. In total you will have 5 labs. Three of which will be focused on Automatic Speech Recognition and two on Text-to-Speech models. Each lab will last two hours and consist of two parts:\n",
    "* Reading Part\n",
    "* Coding Part \n",
    "\n",
    "In each part you might find question or tasks/activities to complete. The grading of the labs is explained below.\n",
    "\n",
    "LAB 3/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What will you learn in LAB 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What tensors are and how scalars, vectors, matrices, and higher-dimensional tensors are represented in practice.\n",
    "* How tensor shapes encode meaning in ASR systems (e.g. batch size, time steps, feature dimensions).\n",
    "* How to create, inspect, and manipulate tensors using common operations such as reshaping, transposing, and adding or removing dimensions.\n",
    "* How typical ASR input and output tensors are structured, and why matching expected shapes is critical for models and losses.\n",
    "* How common tensor shape mistakes arise in ASR pipelines (e.g. missing batch dimensions, swapped axes) and how to debug them.\n",
    "* How underfitting and overfitting appear during ASR training, and how fine-tuning pre-trained models can help balance generalization and memorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Are Tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor is a container that holds numbers.\n",
    "Nothing more. Nothing magical.\n",
    "What makes tensors special is that they can have many dimensions and a shape that tells us how the numbers are organized.\n",
    "In deep learning (and ASR), everything is a tensor: audio, features, model outputs, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from speechbrain.inference.ASR import EncoderDecoderASR\n",
    "from torch.utils.data import random_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars ‚Äî a tensor with no dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scalar is a single number.\n",
    "It has no axes, only a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1400)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.14)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "# [] means: no dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors ‚Äî one-dimensional tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector is a list of numbers.\n",
    "It has one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(v)\n",
    "print(v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "What is a dimension of a tensor? Explain in your own words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "# A dimension of a tensor is one direction or axis along which data is organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a full tensor example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[10., 11., 12.],\n",
      "         [13., 14., 15.],\n",
      "         [16., 17., 18.]]])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [[1., 2., 3.],\n",
    "     [4., 5., 6.],\n",
    "     [7., 8., 9.]],\n",
    "\n",
    "    [[10., 11., 12.],\n",
    "     [13., 14., 15.],\n",
    "     [16., 17., 18.]]\n",
    "])\n",
    "\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: \n",
    "\n",
    "Explain what each number of torch.size output represente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "the the number of elements across each dimension. i.e 2, 3x3 matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices ‚Äî two-dimensional tensors\n",
    "\n",
    "A matrix is a table of numbers with rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0]\n",
    "])\n",
    "print(m)\n",
    "print(m.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors ‚Äî more than two dimensions\n",
    "\n",
    "When we go beyond tables, we usually say tensor.\n",
    "In ASR, tensors often have three dimensions or more.\n",
    "\n",
    "Typical ASR tensor:\n",
    "\n",
    "(batch, time, features)\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 300, 80])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, 300, 80)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning:\n",
    "\n",
    "* 8 audio files (batch)\n",
    "* 300 time frames\n",
    "* 80 features per frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The batch dimension (very important)\n",
    "\n",
    "Neural networks are built to process many examples at the same time, not one by one. Doing things in parallel is much faster on GPUs and modern CPUs.\n",
    "\n",
    "Instead of giving the model:\n",
    "* ‚Äúhere is one audio file‚Äù\n",
    "\n",
    "we give it:\n",
    "* ‚Äúhere are many audio files at once‚Äù\n",
    "\n",
    "The batch dimension tells the model:\n",
    "‚ÄúHow many examples am I processing at the same time?‚Äù\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question \n",
    "\n",
    "Create a tensor with 12 audio files, 500 time frames and 90 features per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 500, 90])\n"
     ]
    }
   ],
   "source": [
    "# Code here\n",
    "\n",
    "x = torch.randn(12, 500, 90)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping ‚Äî same numbers, different shape\n",
    "\n",
    "Reshaping changes how numbers are grouped, not the numbers themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 300, 80])\n",
      "torch.Size([8, 24000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, 300, 80)\n",
    "print(x.shape)\n",
    "y = x.reshape(8, 24000)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposing ‚Äî swapping dimensions\n",
    "\n",
    "Sometimes models expect dimensions in a different order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 300, 80])\n",
      "torch.Size([8, 80, 300])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, 300, 80)\n",
    "print(x.shape)\n",
    "y = x.transpose(1, 2)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze and Unsqueeze ‚Äî adding or removing size-1 dimensions\n",
    "\n",
    "Neural networks often expect tensors to have exact dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300, 80])\n",
      "torch.Size([300, 80])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 300, 80)\n",
    "print(x.shape)\n",
    "\n",
    "y = x.squeeze(0)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "squeeze removed a dimension, read the documentation and find a function to add a dimension: https://docs.pytorch.org/docs/stable/torch.html\n",
    "\n",
    "add a dimension to the tensor below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300, 80])\n",
      "torch.Size([1, 1, 300, 80])\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "x = torch.randn(1, 300, 80)\n",
    "print(x.shape)\n",
    "\n",
    "y = x.unsqueeze(0)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why tensor shapes cause most ASR bugs\n",
    "\n",
    "In ASR, models expect very specific shapes.\n",
    "If the shape is wrong, training crashes or gives nonsense results.\n",
    "\n",
    "Common mistakes:\n",
    "\n",
    "* Missing batch dimension\n",
    "* Time and feature axes swapped\n",
    "* Labels shape not matching predictions\n",
    "\n",
    "\n",
    "Golden debugging rule:\n",
    "\n",
    "print(tensor.shape)\n",
    "\n",
    "üëâ If you don‚Äôt know the shape, you don‚Äôt know what your code is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Below is a toy example of a model that has a bug. Debug it and fix the bug using one of the tensor manipulation techniques from this lesson.\n",
    "\n",
    "Reread the lesson again if you neeed help fixing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINT:\n",
    "\n",
    "In ASR, most bugs happen at these boundaries:\n",
    "\n",
    "* After feature extraction\n",
    "* Before model forward\n",
    "* Before loss / decoding\n",
    "\n",
    "print the shapes!\n",
    "\n",
    "* A lot of ASR bugs are literally ‚Äú2D vs 3D‚Äù.\n",
    "\n",
    "print(\"dims:\", features.dim(), \"shape:\", features.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 300, 30])\n"
     ]
    }
   ],
   "source": [
    "class SimpleASR(nn.Module):\n",
    "    def __init__(self, n_features=80, hidden=64, n_classes=30):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(n_features, hidden, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(\n",
    "                f\"Expected input shape (B, T, F), got {x.shape}\"\n",
    "            )\n",
    "        y, _ = self.rnn(x)\n",
    "        return self.linear(y)\n",
    "\n",
    "model = SimpleASR()\n",
    "\n",
    "features = torch.randn(300, 80) \n",
    "features = features.unsqueeze(0)\n",
    "\n",
    "\n",
    "out = model(features)             \n",
    "print(\"Output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Explain what was the bug in your own words, and what steps you took to debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer\n",
    "# The input tensor must have three dimensions: (B, T, F), but only 2 were provided. to fix it we need to add a batch dimension\n",
    "#  using unsqueeze(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer\n",
    "\n",
    "Think of a Linear layer as a very simple machine with two rules.\n",
    "\n",
    "* The first rule is: how many numbers go in.\n",
    "* The second rule is: how many numbers come out.\n",
    "\n",
    "When you write nn.Linear(24000, 10), you are telling the machine: ‚ÄúI will give you 24000 numbers at a time, and you must give me back 10 numbers.‚Äù The machine does not care how many examples you give it at once ‚Äî that part is handled automatically. It only checks the last group of numbers. So if you give it one example, it expects (24000) numbers; if you give it many examples, it expects (batch, 24000) numbers. After the Linear layer, each example now has 10 numbers instead of 24000, so the output shape becomes (batch, 10). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 300, 80])\n",
      "Input shape: torch.Size([4, 24000])\n",
      "Output shape: torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "class UtteranceClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(300 * 80, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = UtteranceClassifier()\n",
    "\n",
    "\n",
    "features = torch.randn(4, 300, 80)\n",
    "print(\"Input shape:\", features.shape)\n",
    "features = features.flatten(start_dim=1)\n",
    "print(\"Input shape:\", features.shape)\n",
    "\n",
    "out = model(features)\n",
    "print(\"Output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Above is another toy code with a big, identify and fix it using tensor manipulation techniques from this lesson. Below, describe how you debugged it and solved it. \n",
    "Describe everything: what linear layer expected, what you gave it, what caused an error and so on.\n",
    "\n",
    "HINT:\n",
    "A Linear layer never understands tables ‚Äî it only understands rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Your answer\n",
    "# The linear layer nn.Linear(300√ó80, 10) expects a 2-D tensor of shape (batch_size, input_features), i.e. (B, 24000).\n",
    "# However, the input tensor had shape (B, 300, 80), which is 3-D and represents a table (time √ó features) per example. \n",
    "# The bug was fixed by flattening the last two dimensions to obtain (B, 300¬∑80), which matches the expected input of the linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Below is another code with an error, for this code **use the audio file abailable in lab_3 folder**. Find the error and describe your debugging process. Keep in mind that these errors are induced bu me but they **often** occur when working with 3D data and asr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbaalbaki/SpeechRecognition/ASR-TTS-Course/.venv/lib/python3.11/site-packages/speechbrain/processing/features.py:1529: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "/home/jbaalbaki/SpeechRecognition/ASR-TTS-Course/.venv/lib/python3.11/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample rate: 16000\n",
      "Loaded waveform shape (channels, time): torch.Size([1, 78480])\n",
      "\n",
      "Transcript:\n",
      "(['HE BEGAN A CONFUSED COMPLAINT AGAINST THE WIZARD WHO HAD VANISHED BEHIND THE CURTAIN ON THE LEFT'], [[16, 494, 5, 106, 42, 82, 4, 401, 172, 40, 7, 204, 65, 2, 226, 202, 200, 110, 35, 3, 325, 15, 186, 4, 666, 2, 619, 469, 39, 2, 402]])\n"
     ]
    }
   ],
   "source": [
    "asr = EncoderDecoderASR.from_hparams(\n",
    "    source=\"speechbrain/asr-crdnn-rnnlm-librispeech\",\n",
    "    savedir=\"pretrained_models/asr-crdnn-rnnlm-librispeech\",\n",
    ")\n",
    "\n",
    "audio_path = \"/home/jbaalbaki/SpeechRecognition/ASR-TTS-Course/STEP2_Do_Your_Labs/Lab_3/audio.wav\"  \n",
    "wav, sr = torchaudio.load(audio_path)\n",
    "wav = wav.mean(dim = 0, keepdim=True)\n",
    "print(\"Loaded sample rate:\", sr)\n",
    "print(\"Loaded waveform shape (channels, time):\", wav.shape)\n",
    "\n",
    "if wav.shape[0] != 1:\n",
    "    raise ValueError(\n",
    "        f\"Got {wav.shape} (channels={wav.shape[0]}). \"\n",
    "    )\n",
    "\n",
    "\n",
    "wav_batch = wav \n",
    "wav_lens = torch.ones(wav.shape[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    text = asr.transcribe_batch(wav_batch, wav_lens)\n",
    "\n",
    "print(\"\\nTranscript:\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tunning\n",
    "\n",
    "Fine-tuning means starting from a model that already knows general speech patterns (for example, wav2vec trained on thousands of hours of audio) and then slightly adjusting it on your own smaller dataset so it adapts to your task, instead of learning everything from scratch. In practice, this is done by freezing most of the pre-trained model and training only a small part (like the output layer), or by unfreezing layers gradually with a small learning rate so the model does not forget what it already knows; if you train too much or unfreeze too many layers with little data, the model will overfit and simply memorize the training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a Wav2vec Model for Automatic Speech Recognition\n",
    "\n",
    "The ASR system used in the first two labs relied on an acoustic model made up of two main components: a *feature* extractor (based on the [wav2vec](https://arxiv.org/pdf/2006.11477) architecture) and a *classifier* (a linear layer). The model then operates in two stages:\n",
    "\n",
    "1. The speech waveform is passed to the feature extractor to transform it into a latent representation (= the *feature maps*).\n",
    "2. The latent representation is passed to a classification layer to compute the probability of each character (= the *emission matrix*).\n",
    "\n",
    "<center><img src=\"https://github.com/magronp/magronp.github.io/blob/master/images/wav2vec2asr.png?raw=true\" width=\"800\"></center>\n",
    "\n",
    "To train such a model in practice, a two-stage proces is used. First, the wav2vec model alone is pre-trained in a self-supervised manner (using speech data only). Then, the classification layer is added and the whole model \\{wav2vec+classifier\\} is fine-tuned in a supervised manner (from speech+transcript data).\n",
    "\n",
    "In the previous labs, we have used a whole model that was already pre-trained and fine-tuned for ASR. In this lab, we start from the pre-trained wav2vec model, and we reproduce the process of fine-tuning it for ASR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import IPython\n",
    "import os\n",
    "import fnmatch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0);\n",
    "\n",
    "MAX_FILES = 100 # lower this number for processing a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset path - If needed, you can change it HERE but NOWHERE ELSE in the notebook!\n",
    "data_dir = \"/home/jbaalbaki/SpeechRecognition/ASR-TTS-Course/asr-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech and transcripts sub-directories paths\n",
    "data_speech_dir = os.path.join(data_dir, 'speech')\n",
    "data_transc_dir = os.path.join(data_dir, 'transcription')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file path: /home/jbaalbaki/SpeechRecognition/ASR-TTS-Course/asr-dataset/speech/61-70968-0001.wav\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load an example audio file and plot a waveform the way we did in class 1\n",
    "audio_file = '61-70968-0001.wav'\n",
    "audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "print(f\"Audio file path: {audio_file_path}\")\n",
    "\n",
    "waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "IPython.display.Audio(data=waveform, rate=sr)\n",
    "\n",
    "plt.figure()\n",
    "xt = torch.arange(waveform.size(1)) / sr\n",
    "plt.plot(xt, waveform.T)\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give|not|so|earnest|a|mind|to|these|mummeries|child|\n"
     ]
    }
   ],
   "source": [
    "# Write a function to get true transcript. Use the previous labs to help you\n",
    "def get_true_transcript(transc_file_path):\n",
    "    with open(transc_file_path, \"r\") as f:\n",
    "        true_transcript = f.read()\n",
    "    true_transcript = true_transcript.lower().replace('\\n', ' ').replace(' ', '|').strip()\n",
    "    return true_transcript\n",
    "# Load and display the true transcription\n",
    "transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "true_transcript = get_true_transcript(transc_file_path)\n",
    "print(true_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide the list of labels (=characters) that can be found in the dataset\n",
    "labels = ['-', '|', 'e', 't', 'a', 'o', 'n', 'i', 'h', 's', 'r', 'd', 'l', 'u', 'm',\n",
    "          'w', 'c', 'f', 'g', 'y', 'p', 'b', 'v', 'k', \"'\", 'x', 'j', 'q', 'z']\n",
    "n_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform the true transcript into a list of integers in order to feed it to a training loss function. To that end, we define a dictionary `dico_labels` that maps each character in the list of possible labels to an integer (for instance, `dico_labels['e']=2` or `dico_labels['a']=4`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the 'labels' list above, define this dictionary\n",
    "dico_labels = {}\n",
    "for index, element in enumerate(labels):\n",
    "    dico_labels[element] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18,  7, 22,  2,  1,  6,  5,  3,  1,  9,  5,  1,  2,  4, 10,  6,  2,  9,\n",
      "         3,  1,  4,  1, 14,  7,  6, 11,  1,  3,  5,  1,  3,  8,  2,  9,  2,  1,\n",
      "        14, 13, 14, 14,  2, 10,  7,  2,  9,  1, 16,  8,  7, 12, 11,  1])\n"
     ]
    }
   ],
   "source": [
    "# Apply dico_labels to the true transcript, and build a tensor from it\n",
    "target_indices = [dico_labels[c] for c in true_transcript]\n",
    "target_indices = torch.tensor(target_indices, dtype=torch.long)\n",
    "print(target_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acoustic model\n",
    "\n",
    "Torchaudio comprises many models whose pretrained weights can be loaded directly (the list can be found [here](https://pytorch.org/audio/stable/pipelines.html#id3)). Here we use `WAV2VEC2_BASE`, which is pre-trained on speech data, but not fine-tuned for ASR.\n",
    "\n",
    "We can apply it to the waveform to compute the feature maps, which is a tensor of size `[1, time steps, feature dim]` (recall that `1` corresponds to the batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Model(\n",
      "  (feature_extractor): FeatureExtractor(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): ConvLayerBlock(\n",
      "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "      )\n",
      "      (1-4): 4 x ConvLayerBlock(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      )\n",
      "      (5-6): 2 x ConvLayerBlock(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (feature_projection): FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
      "        (conv): ParametrizedConv1d(\n",
      "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (aux): Linear(in_features=768, out_features=29, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the acoustic model: here it's a Wav2vec base that is not fine-tuned. Use previous labs to help you\n",
    "model_name = 'WAV2VEC2_ASR_BASE_100H'\n",
    "bundle = getattr(torchaudio.pipelines, model_name)\n",
    "acoustic_model = bundle.get_model()\n",
    "\n",
    "# Display model architecture\n",
    "print(acoustic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 180, 29])\n"
     ]
    }
   ],
   "source": [
    "# Compute the feature maps\n",
    "with torch.inference_mode():\n",
    "    features, _ = acoustic_model(waveform)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model includes a wav2vec feature extractor (so it should be re-instaciated in the `__init__` method) and a linear classification layer that takes the feature maps and outputs log-probabilities per class (thus it has to use a [log softmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) activation after classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2vecASR(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.wav2vec = getattr(torchaudio.pipelines, 'WAV2VEC2_BASE').get_model()\n",
    "        self.feature_dim = self.wav2vec.encoder.feature_projection.projection.out_features\n",
    "        self.output_layer = nn.Linear(self.feature_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features, _ = self.wav2vec(x)\n",
    "        emission = self.output_layer(features)\n",
    "        emission = emission.log_softmax(dim=-1)\n",
    "        return emission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960.pth\" to /home/jbaalbaki/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 360M/360M [00:18<00:00, 20.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Instanciate the model\n",
    "output_size = n_labels\n",
    "model = Wav2vecASR(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 94393245\n"
     ]
    }
   ],
   "source": [
    "# A function to count the number of trainable parameters\n",
    "def count_tlearnable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Number of trainable parameters:', count_tlearnable_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is very large (90M+ parameters) mostly because of the wav2vec part. To speed training in this lab, we freeze the wav2vec part and we only train the classification layer. To that end, we need to set `requires_grad = False` for the wav2vec's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to freeze parameters, apply it to \"wav2vec\" part of the model,\n",
    "# and print the new number of trainable parameters\n",
    "def freeze_params(m):\n",
    "    for param in m.parameters():\n",
    "        param.requires_grad = False\n",
    "    return\n",
    "    \n",
    "model.wav2vec.apply(freeze_params)\n",
    "\n",
    "print('Number of trainable parameters:', count_tlearnable_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization function for the network's parameters\n",
    "def init_params(m, seed=0):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data, generator=torch.manual_seed(seed))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_layer.apply(init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    emission = model(waveform)\n",
    "print(emission.shape)\n",
    "\n",
    "# Vizualize the emission matrix\n",
    "plt.figure()\n",
    "plt.imshow(emission[0].cpu().T)\n",
    "plt.title(\"Emission matrix\")\n",
    "plt.xlabel(\"Frame (time-axis)\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide a function to get a transcript from the emission matrix (similar to the greedy decoder from lab 1)\n",
    "def transcript_from_emission(emission, labels):\n",
    "    indices = torch.argmax(emission, dim=-1)  # take the most likely index at each time step\n",
    "    indices = torch.unique_consecutive(indices, dim=-1) # remove duplicates\n",
    "    indices = [i for i in indices if i != 0] # remove the blank token\n",
    "    transcript = \"\".join([labels[i] for i in indices]) # convert integers back into characters\n",
    "    transcript = transcript.lower()\n",
    "    return transcript\n",
    "\n",
    "# The transcript using a non-trained model should look bad\n",
    "print(transcript_from_emission(emission[0], labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We now need to define the `Dataset` class, to efficiently load the speech data and true transcript (in the form of indices / integers).\n",
    "\n",
    "The `__getitem__` method returns three outputs:\n",
    "\n",
    "- `waveform`: a tensor containing the speech waveform\n",
    "- `true_transcript`: the true transcript as per using the provided loading function\n",
    "- `target_indices`: a tensor containing the integers corresponding to the transcript\n",
    "\n",
    "The `__init__` method should make use of the `MAX_FILES` variable to limit the size of the dataset (for speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR Dataset class\n",
    "class ASRdataset(Dataset):\n",
    "    def __init__(self, data_speech_dir, data_transc_dir, dico_labels, MAX_FILES=None):\n",
    "        self.data_speech_dir = data_speech_dir\n",
    "        self.data_transc_dir = data_transc_dir\n",
    "        self.audio_files = self._find_files(data_speech_dir)[:MAX_FILES]\n",
    "        self.dico_labels = dico_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load the waveform\n",
    "        audio_file = self.audio_files[index]\n",
    "        audio_file_path = os.path.join(self.data_speech_dir, audio_file)\n",
    "        waveform, _ = torchaudio.load(audio_file_path, channels_first=True)\n",
    "        # load the true transcript\n",
    "        transc_file_path = os.path.join(self.data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "        true_transcript = get_true_transcript(transc_file_path)\n",
    "        # transform transcript into list of integers\n",
    "        target_indices = [self.dico_labels[c] for c in true_transcript]\n",
    "        target_indices = torch.tensor(target_indices, dtype=torch.long)\n",
    "        return waveform, true_transcript, target_indices\n",
    "\n",
    "    def _find_files(self, directory, pattern='*.wav'):\n",
    "        \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "        files = []\n",
    "        for root, _, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, pattern):\n",
    "                files.append(filename)\n",
    "        files = sorted(files)\n",
    "        return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the ASR dataset and print its length\n",
    "asrdataset = ASRdataset(data_speech_dir, data_transc_dir, dico_labels, MAX_FILES=MAX_FILES)\n",
    "print('Dataset length:', len(asrdataset))\n",
    "\n",
    "# Get the first data sample, and print some information\n",
    "waveform, true_transcript, target_indices = asrdataset[0]\n",
    "print(waveform.shape)\n",
    "print(true_transcript)\n",
    "print(target_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-life applications (and as we usually do in the \"Neural Networks\" labs), we assemble the data samples into *batches* for efficiency. However, our data points here have different lenghts in general: two speech waveforms / two transcripts are not guaranteed to have the same duration / number of characters. Therefore, in such a case we need to customize the dataloader such that it performs some [padding operation](https://www.codefull.org/2018/11/use-pytorchs-dataloader-with-variable-length-sequences-for-lstm-gru/) in order to yield data samples of same length. However, to keep things simple in this lab, we skip this padding operation, and we do not work with batches but rather iterate over the dataset directly.\n",
    "\n",
    "## Training with the CTC loss\n",
    "\n",
    "For a given data sample in the dataset, we have:\n",
    "- the emission matrix (log-probability of each character over time frames)\n",
    "- the true transcript (represented as a list of integers corresponding to each character)\n",
    "\n",
    "However, to train our network, we need to obtain an estimated transcript so that we can compute a loss between the true and estimated transcripts. This requires some post-processing of the emission matrix, but the good news is that we don't have to do it, the [CTC loss](https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html) handles that for us!\n",
    "\n",
    "In lab 2 we have use the CTC algorithm to perform inference, but it can also be used as a loss function to train an ASR network. Not only the CTC loss handles the post-processing from the emission matrix, but its great advantage is that it performs alignment from input/output pairs of different lengths, so we don't have to explicitly align each character in the transcript with a time frame in the emission matrix.\n",
    "\n",
    "In Pytorch, [CTC loss](https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html) is fed with the the emission matrix and the tensor containing target indices (corresponding to the true transcript). We also need to give it the input and target lengths explicitly: indeed, even though here we don't manipulate batches / we don't do padding, in general this would be the case so we need to let the function know what is the actual input/target length (before padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the emission matrix (not in inference mode such that we keep track of the gradients)\n",
    "emission = model(waveform)\n",
    "emission = emission[0] #remove the \"batch\" dimension (since here batch_size=1)\n",
    "\n",
    "# Define the input and target lengths as tensors\n",
    "T = emission.shape[0]\n",
    "L = len(target_indices)\n",
    "input_length = torch.tensor(T, dtype=torch.long)\n",
    "target_length = torch.tensor(L, dtype=torch.long)\n",
    "\n",
    "# Instanciate a loss object\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = ctc_loss(emission, target_indices, input_length, target_length)\n",
    "print(loss.item())\n",
    "\n",
    "# Compute the gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `training_wav2vecASR` is used to train the model. It is similar to what you usually do in the Neural Networks labs, although here you do not build batches of data (thus we don't need a `Dataloader` object). Instead, you directly iterate over the `Dataset`. The training function uses an Adam optimizer, and no validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function\n",
    "def training_wav2vecASR(model, train_dataloader, num_epochs, loss_fn, learning_rate):\n",
    "\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    model_tr.train()\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tr_loss = 0\n",
    "        for data_sample in asrdataset:\n",
    "            \n",
    "            # Get the data\n",
    "            waveform, true_trans, target_indices = data_sample\n",
    "            \n",
    "            # Apply the model\n",
    "            emission = model_tr(waveform)\n",
    "            emission = emission.squeeze()\n",
    "            \n",
    "            #print(true_trans)\n",
    "            #print(transcript_from_emission(emission, labels))\n",
    "\n",
    "            # Get the input and target lengths\n",
    "            input_length = torch.tensor(emission.shape[0], dtype=torch.long)\n",
    "            target_length = torch.tensor(len(target_indices), dtype=torch.long)\n",
    "    \n",
    "            # Compute the CTC loss\n",
    "            loss = loss_fn(emission, target_indices, input_length, target_length)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            tr_loss += loss.item()\n",
    "    \n",
    "        # Normalize and store loss\n",
    "        tr_loss = tr_loss / len(asrdataset)\n",
    "        train_losses.append(tr_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {tr_loss:.4f}\")\n",
    "\n",
    "    return model_tr, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters (only one epoch for speed)\n",
    "num_epochs = 1\n",
    "learning_rate = 0.1\n",
    "loss_fn = nn.CTCLoss()\n",
    "\n",
    "model_tr, train_losses = training_wav2vecASR(model, asrdataset, num_epochs, loss_fn, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: \n",
    "\n",
    "What do you observe about the training process? Is it fast or slow? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting on one sample\n",
    "\n",
    "Even when the training is slow, we can assess that our training pipeline works properly by conducting training on a unique sample, and checking the transcript on the same sample. This technique (= overfitting on one sample) is useful to \"crash test\" if everything runs and if our model / training pipeline has any chance to work on a larger dataset.\n",
    "\n",
    "We will instanciate a model from scratch, freeze the wav2part, and initialize the classification layer. Build a dataset made up of 1 sample (use the `MAX_FILES` parameter), and conduct training for 50 epoch using this dataset. Once the model is trained, you will compute the emission matrix from this sample's waveform and the estimated transcript. Display the true and estimated transcripts. Also do the same on another sentence so that you can assess that the model is not able to generalize properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a model, freeze the wav2vec part, and initialize the classifier\n",
    "model = Wav2vecASR(n_labels)\n",
    "model.wav2vec.apply(freeze_params)\n",
    "model.output_layer.apply(init_params)\n",
    "\n",
    "# Build a very small dataset\n",
    "asrdataset = ASRdataset(data_speech_dir, data_transc_dir, dico_labels, MAX_FILES=1)\n",
    "\n",
    "# Training\n",
    "num_epochs = 50\n",
    "model_tr, train_losses = training_wav2vecASR(model, asrdataset, num_epochs, loss_fn, learning_rate)\n",
    "\n",
    "# Save the model's parameters\n",
    "torch.save(model_tr.state_dict(), 'model_wav2vecASR.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data sample\n",
    "waveform, true_transcript, _ = asrdataset[0]\n",
    "\n",
    "# Instanciate the model and load the trained parameters\n",
    "model = Wav2vecASR(n_labels)\n",
    "model.load_state_dict(torch.load('model_wav2vecASR.pt', weights_only=True))\n",
    "\n",
    "# Apply the model and get the estimated transcript\n",
    "with torch.inference_mode():\n",
    "    emission = model(waveform)\n",
    "    est_transcript = transcript_from_emission(emission[0], labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display true and estimated transcripts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it on a different sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display true and estimated transcripts here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "When training the model how do you know if the model is learning properly, what do you code and inspect? Explain in depth.\n",
    "Documentation to read: https://medium.com/data-science/learning-curve-to-identify-overfitting-underfitting-problems-133177f38df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a new function to be able to split the dataset into train and val. To choose a good validation split, the validation data must be completely unseen during training and representative of the same task and conditions as the training data; a common rule of thumb is to keep about 10‚Äì30% of the data for validation, making sure it contains different speakers or utterances when possible, and to use it to monitor when validation performance stops improving, which tells you when to stop fine-tuning or adjust your hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_wav2vecASR2(model, train_dataset, val_dataset, num_epochs, loss_fn, learning_rate):\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    model_tr.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tr_loss = 0.0\n",
    "\n",
    "        for waveform, true_trans, target_indices in train_dataset:\n",
    "            emission = model_tr(waveform)\n",
    "            emission = emission.squeeze()\n",
    "\n",
    "            input_length = torch.tensor(emission.shape[0], dtype=torch.long)\n",
    "            target_length = torch.tensor(len(target_indices), dtype=torch.long)\n",
    "\n",
    "            loss = loss_fn(emission, target_indices, input_length, target_length)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        tr_loss = tr_loss / max(1, len(train_dataset))\n",
    "        train_losses.append(tr_loss)\n",
    "\n",
    "        # validation curve\n",
    "        vl_loss = eval_ctc_loss(model_tr, val_dataset, loss_fn)\n",
    "        val_losses.append(vl_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]  Train: {tr_loss:.4f}  Val: {vl_loss:.4f}\")\n",
    "\n",
    "    return model_tr, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ctc_loss(model, dataset, loss_fn):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for waveform, true_trans, target_indices in dataset:\n",
    "            emission = model(waveform)  \n",
    "\n",
    "            emission = emission.squeeze()\n",
    "\n",
    "            input_length = torch.tensor(emission.shape[0], dtype=torch.long)\n",
    "            target_length = torch.tensor(len(target_indices), dtype=torch.long)\n",
    "\n",
    "            loss = loss_fn(emission, target_indices, input_length, target_length)\n",
    "            total += loss.item()\n",
    "\n",
    "    model.train()\n",
    "    return total / max(1, len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FILES = 50  \n",
    "asrdataset = ASRdataset(data_speech_dir, data_transc_dir, dico_labels, MAX_FILES=MAX_FILES)\n",
    "\n",
    "val_ratio = 0.2\n",
    "n_total = len(asrdataset)\n",
    "n_val = max(1, int(n_total * val_ratio))\n",
    "n_train = n_total - n_val\n",
    "\n",
    "train_ds, val_ds = random_split(asrdataset, [n_train, n_val])\n",
    "\n",
    "print(\"Train:\", len(train_ds), \"Val:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tr, train_losses, val_losses = training_wav2vecASR(\n",
    "    model, train_ds, val_ds, num_epochs=50, loss_fn=loss_fn, learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Train loss\")\n",
    "plt.plot(val_losses, label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"CTC loss\")\n",
    "plt.title(\"Overfitting demo: train vs validation\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Experiment with MAX_FILES and tell me any changes you observe in the graph. Why do they occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Experiment with validation split and tell me any changes you observe in the graph. Why do they occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, **optimizers** are the algorithms that decide how a model‚Äôs parameters are updated during training, and different optimizers behave differently (for example, SGD, Adam, RMSprop). In this course we use **Adam**, which is a popular choice for fine-tuning because it adapts the learning step automatically for each parameter, but it also exposes several **tunable hyperparameters** that strongly affect training behavior. During fine-tuning, these parameters can change how fast the model learns, how stable training is, and how much the model overfits. **Read the official Adam documentation** and identify which parameters can be adjusted (such as those controlling step size, regularization, and gradient smoothing), and to think about how changing them would affect training and validation curves during fine-tuning.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2023/12/adam-optimizer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Lits Adam's parameters that can be fine-tuned and what they are responsible for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Experiment with one of Adam's parameters, describe the outcome and any changes you observe in the graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Material on TDNN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TDNN: https://kaleidoescape.github.io/tdnn/\n",
    "\n",
    "CNN: https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939/\n",
    "\n",
    "IN SHORT: \n",
    "\n",
    "A CNN (Convolutional Neural Network) is a model that learns by looking at small pieces of data at a time and reusing the same pattern-detector everywhere, like sliding a small window along the data to spot useful patterns. A TDNN (Time-Delay Neural Network) does the same thing, but specifically for data that changes over time, such as speech: it looks at a few time steps before and after the current moment to understand what is happening now. In practice, a TDNN is simply a 1-dimensional CNN applied along the time axis, designed to recognize temporal patterns while being insensitive to small time shifts.\n",
    "\n",
    "A one-dimensional CNN (1D CNN) is a neural network that looks for patterns along a single line, not across a 2-D surface like an image. Imagine a row of numbers changing over time, such as a speech signal, sound features, or a sentence represented as numbers. A 1D CNN uses a small moving window that slides left to right along this row and checks each small chunk for patterns (for example, a short sound shape or a word pattern). The same detector is reused everywhere, so the network learns to recognize a pattern no matter where it appears in time. This makes 1D CNNs good at handling sequences like audio, where timing may shift slightly but the pattern itself stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Read the materials on CNN AND TDNN and explain the architecture in your own words, be detailed. What are the advantages of TDNN compared to CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding blocks and questions that will be tested next class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSPECTING TENSOR SHAPE\n",
    "x = torch.randn(300, 80)\n",
    "print(\"dims:\", x.dim())\n",
    "print(\"shape:\", x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING DIMENSION\n",
    "x = torch.randn(300, 80)\n",
    "x = x.unsqueeze(0)\n",
    "print(x.shape)  # (1, 300, 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is a tensor, in simple words?\n",
    "* Give me an example tensor used as input into the linear layer of an asr system and explain the contents.\n",
    "* Give me three different ways to manipulate tensors and explain what they do.\n",
    "* What is overfitting, and how does it appear on training vs validation loss curves?\n",
    "* What is underfitting, and how do its loss curves look?\n",
    "* Explain how Adam optimizer works in short, What is the role of it in neural network training?\n",
    "* What parameters does Adam optimizer have, what are they responsible for?\n",
    "* What is the benefit of the TDNN architecture compared to a conventional CNN? Explain both architectures. (You will need to google for this one or refer to your lecture slides)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
